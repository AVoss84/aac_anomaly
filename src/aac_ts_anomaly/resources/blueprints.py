import pandas as pd
import numpy as np
from copy import deepcopy
import glob as gl
import subprocess, os
from datetime import datetime, date
import dateutil.parser as dateparser
from aac_ts_anomaly.utils import utils_func as utils
from aac_ts_anomaly.config import global_config as glob
from aac_ts_anomaly.resources import config
from abc import ABC, abstractmethod

class AbstractPreprocessor(ABC):
    """
    Preprocessing class: Slices/aggregates the portfolio into univariate time series
    according to minimal sample size and median counts per series 
    """

    #def __init__(self):           
    #    print("*** Claims Reporter started ***")

    def _convert2date(self, y, t):
         return datetime.strptime(f'{y} {t} 1', '%Y %m %d')  # %G, %V, %u are ISO equivalents of %Y, %W, %w

    def _year_week(self, y, w):
         return datetime.strptime(f'{y} {w} 1', '%G %V %u')  # %G, %V, %u are ISO equivalents of %Y, %W, %w

    def _correct_cweek_53(self, dat : pd.core.frame.DataFrame, time_col : str = 'year_period_ts', verbose : bool = False)-> pd.DataFrame:
            """
            This aggregates non-unique dates which were generated by 
            transforming from calendar week to a representative day 
            of that week (using _year_week function)
            Example: 
            2019-53 : 2019-12-30 and 2020-01: 2019-12-30. 
            These two will be summed up to have unique date points 
            for time series analysis. Result will remove 2019-53 for example
            """
            my_ts = deepcopy(dat)
            ssa = my_ts.groupby([time_col]) 
            #time_duplicates = ssa.agg(size=(target_col,'count')).reset_index()
            deduplicated_ts = ssa.agg(target = (self.target_col,'sum')).reset_index()
            my_ts.drop(columns=[self.target_col], inplace = True)
            my_ts_new = my_ts.merge(deduplicated_ts, how='left', left_on=[time_col], right_on=[time_col])
            my_ts_new.drop_duplicates(subset=[time_col, self.target_col], keep='last', inplace=True)
            my_ts_new.reset_index(inplace=True)
            if verbose: print('{} rows aggregated'.format(my_ts.shape[0]-my_ts_new.shape[0]))
            return my_ts_new

    def q_50(self, x)-> float:
        return x.quantile(0.5)

    def iqr(self, x)-> float:
        return x.quantile(0.75) - x.quantile(0.25)
    
    #@staticmethod
    @abstractmethod
    def upsample_time_series(self, df : pd.DataFrame)-> pd.DataFrame:
        """Upsample time series to have complete cycle, i.e. regular time series

        Args:
            df (pd.DataFrame): _description_
        Returns:
            pd.DataFrame: Corrected input dataframe
        """
        pass
    
    @abstractmethod
    def process_data(self, data_orig : pd.core.frame.DataFrame, 
                    aggreg_level : str = None, agg_func = 'sum', aggreg_dimensions : list = ['Region', 'OE', 'Lob',  'LossCat'],
                    ignore_lag: int = None, min_sample_size: int = None, min_median_target: float = None, 
                    verbose = True):
        """
        Preprocess univariate time series and create generator to yield single time series.

        Arguments:
        ----------
        aggreg_level : string specifying the way the data should be aggregated.
                       Can be one of: all_combi, lob_only, region_only
        agg_func : either string, the aggregation/pooling function used in the groupby statements, 
                   must match one of numpy's fct: np.mean, np.sum, etc., or name of custom aggregation function
        aggreg_dimensions : list of variable names which comprise the maximum number of dimensions
                            over which we will aggregate when using aggreg_level = all_combi
        ignore_lag : integer, number of time points to leave out of the preprocessing/training process
                     at the end of the time series. Mostly only the most recent time point is left out 
        min_sample_size : integer, minimum time series length contraint. If not fullfilled
                          the time series is not used for anomaly detection, but will be 
                          used in an additional coarser aggregation step (single vs. aggreg)
        min_median_target : float value (default: None), If specified is used as additional filter constraint to min_sample_size
                            Defines the lowest possible median target value (q50_target). If median is lower than that
                            the data points are further aggregated (see single vs. aggreg logic -> pooling == 1)                                                          
        """
        pass



class AbstractReportCreator(ABC):    
    """
    Claims anomaly report creator for all use cases, all combi, region, Lob, given a frequency
    """
    def __init__(self, input_file : str = "Monthly Incurred Movements", 
                 output_to : list = ['sftp', 'local'], 
                 verbose : bool = True, **kwargs):
        """
        Arguments:
        input_file: name of the raw input file. will be used to detect the most recent matching file on SFTP
        output_to : write input to either sftp or to the specified folder on local server
        """
        self.verbose = verbose
        self.input_file = input_file
        self.output_to = output_to

        # Currently known file-period mappings ('lookup list'):
        self.input_files_lookup = config.in_out12['input']['known_file_names']    # use monthly yaml here, since the filenames are copied anyway (same as in weekly)

        # Check if provided filename is consistent with lookup list: 
        self.file_distances, self.nearest_file_category = utils.get_filename_NN(input_filename = self.input_file, lookup_dict = self.input_files_lookup)
        
        # Lookup corresponding periodicity for given standard file name:
        self.periodicity = self.input_files_lookup[self.nearest_file_category]
        assert self.output_to[0] in ['sftp', 'local'], 'arg output_to must be sftp or local!'
        if self.output_to[0] == 'sftp':
            self.output_files_dir = glob.UC_REPORT_DIR      # SFTP
        else:
            self.output_files_dir = glob.UC_PWEAVE_DIR      # package folder / local server (i.e. VM or Jupyter)
        try:
            d = dateparser.parse(self.input_file,fuzzy=True)
            filedate = d.strftime("%Y_%m_%d")
        except ValueError as ve:
            print(ve); print("Using today's date instead.")
            filedate = date.today().strftime("%Y_%m_%d")    

        if self.verbose : 
            print('Input file: {} (Periodicity: {})'.format(self.input_file, self.periodicity))
            print('Writing reports to: {}'.format(self.output_files_dir))
            utils.logger_utils.info('Writing reports to: {}'.format(self.output_files_dir))
        
        
        if self.periodicity == 52:
            config_input = config.in_out52['input']
            config_output = config.in_out52['output']
        if self.periodicity == 12:    
            config_input = config.in_out12['input']
            config_output = config.in_out12['output']

        filename = config_output['report_filename']         # without timestamp
        append_this = ''
        filename_new = filename+append_this

        # Create output file name  
        self.output_file_name_region_pdf = filename_new+'_region_only_'+filedate+'.pdf'
        self.output_file_name_all_pdf = filename_new+'_all_combi_'+filedate+'.pdf'
        self.output_file_name_lob_pdf = filename_new+'_lob_only_'+filedate+'.pdf'

        self.filename_lookup = {'all': self.output_file_name_all_pdf,
                           'lob' : self.output_file_name_lob_pdf,
                           'region' : self.output_file_name_region_pdf}

       
    # def create(self, usecase : list = ['all', 'lob', 'region']):        
    #     """
    #     Wrapper function to conveniently run all reports at once if needed. 
    #     """
    #     self.outputs = ()
    #     for uc in usecase:
    #         if uc == 'all':
    #            out_all = self._run_all() ; self.outputs += out_all,
    #            print(out_all) 
    #         elif uc == 'lob':
    #            out_lob = self._run_lob(); self.outputs += out_lob,
    #            print(out_lob)
    #         else:
    #            out_region = self._run_region(); self.outputs += out_region,
    #            print(out_region)
    #     print("All jobs done!")     
    #     return self.outputs        

    def create(self, usecase : list = ['all']):        
        """
        Wrapper function to conveniently run all reports at once if needed. 
        """
        self.outputs = ()

        for uc in usecase:
            if uc == 'all':
               out_all = self._run_all() ; self.outputs += out_all,
               print(out_all) 
            else:
                pass   

        print("All jobs done!")     
        return self.outputs   


    @abstractmethod        
    def _run_all(self, df : pd.DataFrame):
        """
        Creates the actual PDF report (for all LoB/Region/OE combinations) 
        """    
        pass

    