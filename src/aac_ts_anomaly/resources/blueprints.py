import pandas as pd
import numpy as np
from copy import deepcopy
import glob as gl
import subprocess, os, abc
from datetime import datetime, date
import dateutil.parser as dateparser
from typing import (Dict, List, Text, Optional, Any, Callable, Union)
from aac_ts_anomaly.config import global_config as glob
#from aac_ts_anomaly.resources import config
from abc import ABC, abstractmethod, abstractproperty


class AbstractPreprocessor(ABC):
    """
    Preprocessing class: Slices/aggregates the portfolio into univariate time series
    according to minimal sample size and median counts per series 
    """
    #def __init__(self):           
    #    print("*** Claims Reporter started ***")

    def _convert2date(self, y, t):
         return datetime.strptime(f'{y} {t} 1', '%Y %m %d')  # %G, %V, %u are ISO equivalents of %Y, %W, %w

    def _year_week(self, y, w):
         return datetime.strptime(f'{y} {w} 1', '%G %V %u')  # %G, %V, %u are ISO equivalents of %Y, %W, %w

    def _correct_cweek_53(self, dat : pd.DataFrame, time_col : str = 'year_period_ts', verbose : bool = False)-> pd.DataFrame:
            """
            This aggregates non-unique dates which were generated by 
            transforming from calendar week to a representative day 
            of that week (using _year_week function)
            Example: 
            2019-53 : 2019-12-30 and 2020-01: 2019-12-30. 
            These two will be summed up to have unique date points 
            for time series analysis. Result will remove 2019-53 for example
            """
            my_ts = deepcopy(dat)
            ssa = my_ts.groupby([time_col]) 
            #time_duplicates = ssa.agg(size=(target_col,'count')).reset_index()
            deduplicated_ts = ssa.agg(target = (self.target_col,'sum')).reset_index()
            my_ts.drop(columns=[self.target_col], inplace = True)
            my_ts_new = my_ts.merge(deduplicated_ts, how='left', left_on=[time_col], right_on=[time_col])
            my_ts_new.drop_duplicates(subset=[time_col, self.target_col], keep='last', inplace=True)
            my_ts_new.reset_index(inplace=True)
            if verbose: print('{} rows aggregated'.format(my_ts.shape[0]-my_ts_new.shape[0]))
            return my_ts_new

    def q_50(self, x)-> float:
        return x.quantile(0.5)

    def iqr(self, x)-> float:
        return x.quantile(0.75) - x.quantile(0.25)
    
    #@staticmethod
    @abstractmethod
    def upsample_time_series(self, df : pd.DataFrame)-> pd.DataFrame:
        """Upsample time series to have complete cycle, i.e. regular time series

        Args:
            df (pd.DataFrame): _description_
        Returns:
            pd.DataFrame: Corrected input dataframe
        """
        pass
    
    @abstractmethod
    def process_data(self, data_orig : pd.DataFrame, 
                    aggreg_level : str = None, agg_func : str = 'sum', aggreg_dimensions : list = ['Region', 'OE', 'Lob',  'LossCat'],
                    ignore_lag: int = None, min_sample_size: int = None, min_median_target: float = None, 
                    verbose : bool = True):
        """
        Preprocess univariate time series and create generator to yield single time series.

        Arguments:
        ----------
        aggreg_level : string specifying the way the data should be aggregated.
                       Can be one of: all_combi, lob_only, region_only
        agg_func : either string, the aggregation/pooling function used in the groupby statements, 
                   must match one of numpy's fct: np.mean, np.sum, etc., or name of custom aggregation function
        aggreg_dimensions : list of variable names which comprise the maximum number of dimensions
                            over which we will aggregate when using aggreg_level = all_combi
        ignore_lag : integer, number of time points to leave out of the preprocessing/training process
                     at the end of the time series. Mostly only the most recent time point is left out 
        min_sample_size : integer, minimum time series length contraint. If not fullfilled
                          the time series is not used for anomaly detection, but will be 
                          used in an additional coarser aggregation step (single vs. aggreg)
        min_median_target : float value (default: None), If specified is used as additional filter constraint to min_sample_size
                            Defines the lowest possible median target value (q50_target). If median is lower than that
                            the data points are further aggregated (see single vs. aggreg logic -> pooling == 1)                                                          
        """
        pass


class AbstractServices(ABC):
    """Abstract base class to define class blueprint (API)"""
    #__metaclass__ = abc.ABCMeta
    #path : Optional[str] = ""
    #root_path : str = glob.UC_DATA_DIR

    @abstractmethod 
    def doRead(self, **others):
       pass

    @abstractmethod 
    def doWrite(self, X, **others):
       pass 

