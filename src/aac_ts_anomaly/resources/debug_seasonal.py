
import os, warnings
#warnings.filterwarnings("ignore")
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import seaborn as sns
import pandas as pd
#from statsmodels.tsa.stattools import adfuller, kpss, acf, grangercausalitytests
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, month_plot, quarter_plot
import numpy as np
from copy import deepcopy
import datetime
#pd.set_option('display.max_rows', 10**5)
pd.set_option('display.max_columns', 10**5)

from importlib import reload
import adtk
#import pweave     # for markdown reports
#os.chdir("..")
from claims_reporting.utils import tsa_utils as tsa
from claims_reporting.utils import utils_func as util
from claims_reporting.services import file

from claims_reporting.config import global_config as glob
from claims_reporting.services import file
#from claims_reporting.services import base
from claims_reporting.resources import config

reload(tsa)
reload(file)
reload(config)
#reload(base)
reload(glob)


def _convert2date(y, t):
        return datetime.strptime(f'{y} {t} 1', '%Y %m %d')  # %G, %V, %u are ISO equivalents of %Y, %W, %w

def _year_week(y, w):
        return datetime.strptime(f'{y} {w} 1', '%G %V %u')  # %G, %V, %u are ISO equivalents of %Y, %W, %w

def _correct_cweek_53(dat : pd.core.frame.DataFrame, time_col : str = 'year_period_ts', target_col : str = 'clm_cnt', verbose=False):
        """
        This aggregates non-unique dates which were generated by 
        transforming from calendar week to a representative day 
        of that week (using _year_week function)
        Example: 
        2019-53 : 2019-12-30 and 2020-01: 2019-12-30. 
        These two will be summed up to have unique date points 
        for time series analysis. Result will remove 2019-53 for example
        """
        my_ts = deepcopy(dat)
        ssa = my_ts.groupby([time_col]) 
        #time_duplicates = ssa.agg(size=(target_col,'count')).reset_index()
        deduplicated_ts = ssa.agg(target = (target_col,'sum')).reset_index()
        my_ts.drop(columns=[target_col], inplace = True)
        my_ts_new = my_ts.merge(deduplicated_ts, how='left', left_on=[time_col], right_on=[time_col])
        my_ts_new.drop_duplicates(subset=[time_col, target_col], keep='last', inplace=True)
        my_ts_new.reset_index(inplace=True)
        if verbose:
            print('{} rows aggregated'.format(my_ts.shape[0]-my_ts_new.shape[0]))
        return my_ts_new

def q_50(x):
    return x.quantile(0.5)

def iqr(x):
    return x.quantile(0.75) - x.quantile(0.25)

'
#os.getcwd()
#reload(util)

filename = util.get_newest_file(search_for = "AGCS CCO CRA - Monthly Incurred Movements",  src_dir=glob.UC_DATA_DIR)
filename

xls = file.XLSXService(path=filename, root_path=glob.UC_DATA_DIR, dtype= {'time': str}, sheetname='data', index_col=None, header=0)

data_orig = xls.doRead()

data_orig.shape

data_orig.head()
'

##############

from claims_reporting.resources import preprocessor as pre

reload(pre)

config_detect = config.in_out12['detection']
config_detect
outlier_filter = config_detect['training']['outlier_filter']
hyper_para = config_detect['training']['hyper_para']
stat_transform = config_detect['training']['stat_transform']

# Instantiate class:
#--------------------
claims = pre.claims_reporting(ts_col = 'target')

#aggreg_level, pre_filter, ignore_week_lag, min_sample_size, min_median_cnts = list(config_detect['preprocessing'].values())

gen = claims.process_data(data_orig, aggreg_level = 'all_combi', ignore_lag = 1, min_sample_size = 20)

# Get next series
#-------------------
label, sub_set = next(gen)

print(label, sub_set.shape[0])
df = deepcopy(sub_set)

df.head()

"""
ts_index, ts_values = df['time'], df['target']
ts_values.index = pd.to_datetime(ts_index) 

ts_values.index.inferred_freq

from adtk.transformer import ClassicSeasonalDecomposition
from adtk.data import validate_series

s = validate_series(ts_values)

s_transformed = ClassicSeasonalDecomposition().fit_transform(s).rename("Seasonal decomposition residual")

len(np.unique(np.diff(s.index))) > 1

s.index
"""

############
# Training:
############

from claims_reporting.resources import trainer

reload(trainer)

train = trainer.trainer(verbose=False)   # will call prerocessing...
#fitted = train.fit(df = df)

results, results_new = train.run_all(data_orig = data_orig, write_table = False, verbose=True, aggreg_level = 'all_combi')

#out = fitted.predict()
#fitted.anomalies

#y = fitted.val_series
y.head()

df.head()
df.info()

df['time']

"""
transform = stat_transform[0]

if periodicity == 52: 
        df['year_period_ts'] = df.apply(lambda row: _year_week(row.year, row.period), axis=1)
        # Remove calendar week 53 if there! Frequ. = 52 from now on.
        df = _correct_cweek_53(df, time_col = 'year_period_ts', target_col = 'target', verbose=verbose_train)

if periodicity == 12:
        df['year_period_ts'] = df['time'] #df.apply(lambda row: _convert2date(row.year, row.period), axis=1)

df.head()
df.apply(lambda row: _convert2date(row.year, row.period), axis=1)

#self.df['year_week_str'] = self.df.apply(lambda row: row.year_period_ts.strftime('%G-%V'), axis=1)
if transform in ['log', 'diff_log']:
    ts_index, ts_values = df['time'], np.log(1 + df[target_col])   # log transform
else:
    ts_index, ts_values = df['time'], df[target_col]

ts_values.index = ts_index 
val_series = validate_series(ts_values)
val_series.index.inferred_freq

if transform in ['diff', 'diff_log']:
    y_lag = Retrospect(n_steps=2, step_size=1).transform(val_series)
    y_lag.dropna(inplace=True)              
    val_series = validate_series(y_lag["t-0"] - y_lag["t-1"])   # first differences
    
# No cweek 53 allowed in the following due to the following and other subsequent
# specifications in time series methods! 
s_deseasonal = deepcopy(val_series)    # instantiate
s_deseasonal
df

#s_desea_inter =  s_deseasonal.resample(rule = 'MS').mean()     #a lternative to asfreq()
#s_desea_inter.interpolate()
#s_deseasonal.asfreq(freq = 'MS', fill_value = 0.)     # use Month start

model_transf = list(transformers.keys())[0]
transf_hyper_para = transformers[model_transf]

anomaly_transformer = eval(model_transf+"("+"**transf_hyper_para)")

s_deseasonal = anomaly_transformer.fit_transform(val_series)
s_deseasonal

"""





